Lexer for a subset of Haskell programming language.

Important to know:
 - reading line by line is okay because no token can span multiple lines,
   the only tricky part is multiline comments, they must be handled by
   a pushdown automata (PDA).
 - using the maximal munch principle is okay and advised by the Haskell 2010 report.
   However, for demonstration purpose, the lexer panicks when it does not know
   what token to produce, so that you do not get comfortable with it! :)
 - DFA can in princtiple be used on the whole input stream, for example,
   take this line:
     {/ dfdfdfdf {- -}
   here, the `{/` is not matched against the beginning of the block of multiline
   comment token, but initially I would move to the initial state and wait until
   `{-` arrives, then I would move to the state when one part of the pattern has
   matched, and finish on seeing `-}`;

   === THIS IS HOW GREP WORKS! NOT HOW LEX WORKS! I NEED LEX-LIKE BEHAVIOUR

   For that, I maintain the two states in every lexeme recognizer, Final and Failed.
   Failed applies when we cannot say that there is any token that the input is being
   tested against, at the position from where the automaton was called.

 - Explicitly defining the rules in a trnstition table is ugly and wastes memory;
   I can minimize the automaton and have less states. Minimization only reduces
   the amount of states, I still have to define every possible transition rule
   for every one of them. I am using runime checks against some rules instead of
   the explicit table.

Limitations:
 - works with ASCII only (unicode is tricky)
 - the layout feature is not implemented [1]

To build & test at the same time (this project is built as a library and it has no
entry point by itself):
- run ```cargo test```

Inspired by this [2] Thanks to Stas Dzundza! :D

[1] https://www.haskell.org/onlinereport/haskell2010, see section 'Lexical structure'
[2] https://github.com/StasDzundza/PythonLexer
